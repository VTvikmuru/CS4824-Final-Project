import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import TensorDataset, DataLoader
from sentence_transformers import SentenceTransformer

# Stable seed
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Load dataset
dataset = load_dataset("HiTZ/This-is-not-a-dataset")
df = dataset["train"].to_pandas().sample(1000, random_state=SEED)  # sample for speed

sentences = df["sentence"].tolist()
labels = df["label"].astype(int).tolist()

train_texts, test_texts, train_labels, test_labels = train_test_split(
    sentences, labels, test_size=0.2, random_state=SEED
)

#------------------------------------------------------------------------------------------------------

# Load pre-generated fake sentences - generated by pregenerate script
def load_generated_samples(filename="generated_fakes_50.txt"):
    with open(filename, "r", encoding="utf-8") as f:
        samples = [line.strip() for line in f if line.strip()]
    return samples

fakes = load_generated_samples("generated_fakes_50.txt")
fake_labels = [0] * len(fakes)

# Sentence transformer
embedder = SentenceTransformer("all-MiniLM-L6-v2")

def embed_texts(texts):
    return embedder.encode(texts, convert_to_numpy=True)

X_test = embed_texts(test_texts)

#------------------------------------------------------------------------------------------------------

# Improved NN Detector
class ImprovedNN(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, dropout=0.3):
        super(ImprovedNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.drop1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)
        self.relu2 = nn.ReLU()
        self.drop2 = nn.Dropout(dropout)
        self.fc3 = nn.Linear(hidden_dim//2, 2)
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.drop1(x)
        x = self.relu2(self.fc2(x))
        x = self.drop2(x)
        return self.fc3(x)

#------------------------------------------------------------------------------------------------------

# Experiment function
def run_experiment(mix_ratios=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]):
    results = []
    for mix in mix_ratios:
        print(f"\n=== Mix Ratio {mix*100:.0f}% Generator ===")

        # Select portion of fakes
        num_fake = int(len(train_texts) * mix)
        mixed_sentences = train_texts + fakes[:num_fake]
        mixed_labels = train_labels + fake_labels[:num_fake]

        # Embed training data
        X_train = embed_texts(mixed_sentences)

        # Convert to tensors
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
        y_train_tensor = torch.tensor(mixed_labels, dtype=torch.long)
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
        y_test_tensor = torch.tensor(test_labels, dtype=torch.long)

        # Initialize model
        input_dim = X_train.shape[1]
        model = ImprovedNN(input_dim)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)

        # Train
        EPOCHS = 100
        for epoch in range(EPOCHS):
            model.train()
            total_loss = 0.0
            for xb, yb in train_loader:
                optimizer.zero_grad()
                outputs = model(xb)
                loss = criterion(outputs, yb)
                loss.backward()
                optimizer.step()
                total_loss += loss.item() * xb.size(0)
            avg_loss = total_loss / len(train_dataset)
            print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}")

        # Evaluate
        model.eval()
        with torch.no_grad():
            test_outputs = model(X_test_tensor)
            preds = torch.argmax(test_outputs, axis=1).numpy()

        acc = accuracy_score(test_labels, preds)
        print("Accuracy:", acc)
        print(classification_report(test_labels, preds))
        cm = confusion_matrix(test_labels, preds)
        print("Confusion Matrix:\n", cm)

        results.append({"mix": mix, "accuracy": acc})

    return pd.DataFrame(results)

#------------------------------------------------------------------------------------------------------

# Run
results_df = run_experiment()

print("\n=== Final Results ===")
print(results_df)

# Plot results
plt.figure(figsize=(8,6))
plt.plot(results_df["mix"], results_df["accuracy"], marker="o", label="Improved NN")
plt.xlabel("Generator Mix Ratio")
plt.ylabel("Accuracy")
plt.title("Detector Performance vs Generator Mix Ratio")
plt.legend()
plt.grid(True)
plt.show()
